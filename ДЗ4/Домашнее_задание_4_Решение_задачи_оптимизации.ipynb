{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решите простую задачу безусловной оптимизации в двумерном пространстве:  \n",
    "$$f(\\boldsymbol x) = -8x_1 - 16x_2 + x_1^2 + 4x_2^2$$\n",
    "используя два метода:\n",
    " - аналитически (функция квадратичная, выпуклая)\n",
    " - методом градиентного спуска, используя один из методов оптимизации torch.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналитически\n",
    "\n",
    "$ f'_{x_1} = -8 + 2 x_1 = 0 \\quad => x_1 = 4 $ \n",
    "\n",
    "$ f'_{x_2} = -16 + 8 x_2 = 0 \\quad => x_2 = 2 $\n",
    "\n",
    "Матрица Гессе (вторых производных):\n",
    "$\\left(\n",
    "\\begin{array}{cccc}\n",
    "2 & 0 \\\\\n",
    "0 & 8\n",
    "\\end{array}\n",
    "\\right).$ \n",
    "\n",
    "Она положительно определена, значит точка $ (4, 2) $ является точкой минимума.\n",
    "$ f(4, 2) = -32 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Методом градиентного спуска, используя один из методов оптимизации torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, x1 4.800000190734863, x2 3.5999999046325684 loss 225.0, grad x1 2.0, grad x2 64.0\n",
      "iteration 1, x1 4.640000343322754, x2 2.319999933242798 loss -21.12000274658203, grad x1 1.6000003814697266, grad x2 12.799999237060547\n",
      "iteration 2, x1 4.51200008392334, x2 2.063999891281128 loss -31.180805206298828, grad x1 1.2800006866455078, grad x2 2.559999465942383\n",
      "iteration 3, x1 4.409600257873535, x2 2.0127999782562256 loss -31.721466064453125, grad x1 1.0240001678466797, grad x2 0.5119991302490234\n",
      "iteration 4, x1 4.3276801109313965, x2 2.0025599002838135 loss -31.83156967163086, grad x1 0.8192005157470703, grad x2 0.10239982604980469\n",
      "iteration 5, x1 4.262144088745117, x2 2.000511884689331 loss -31.892601013183594, grad x1 0.655360221862793, grad x2 0.020479202270507812\n",
      "iteration 6, x1 4.209715366363525, x2 2.0001022815704346 loss -31.931283950805664, grad x1 0.5242881774902344, grad x2 0.0040950775146484375\n",
      "iteration 7, x1 4.16777229309082, x2 2.0000205039978027 loss -31.95602035522461, grad x1 0.4194307327270508, grad x2 0.0008182525634765625\n",
      "iteration 8, x1 4.134217739105225, x2 2.0000040531158447 loss -31.97185516357422, grad x1 0.3355445861816406, grad x2 0.000164031982421875\n",
      "iteration 9, x1 4.10737419128418, x2 2.0000007152557373 loss -31.981983184814453, grad x1 0.2684354782104492, grad x2 3.24249267578125e-05\n",
      "iteration 10, x1 4.085899353027344, x2 2.000000238418579 loss -31.988475799560547, grad x1 0.21474838256835938, grad x2 5.7220458984375e-06\n",
      "iteration 11, x1 4.068719387054443, x2 2.0 loss -31.992618560791016, grad x1 0.1717987060546875, grad x2 1.9073486328125e-06\n",
      "iteration 12, x1 4.054975509643555, x2 2.0 loss -31.995281219482422, grad x1 0.13743877410888672, grad x2 0.0\n",
      "iteration 13, x1 4.043980598449707, x2 2.0 loss -31.996978759765625, grad x1 0.10995101928710938, grad x2 0.0\n",
      "iteration 14, x1 4.035184383392334, x2 2.0 loss -31.998065948486328, grad x1 0.08796119689941406, grad x2 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def f(x1, x2):\n",
    "    return - 8 * x1 - 16 * x2 + x1 ** 2 + 4 * x2 ** 2\n",
    "\n",
    "x1 = torch.tensor(5, dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor(10, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([x1, x2], lr=0.1)\n",
    "\n",
    "\n",
    "for i in range(15):\n",
    "    y = f(x1, x2)\n",
    "    optimizer.zero_grad()  # у optimizer есть свой zero grad\n",
    "    y.backward()\n",
    "    optimizer.step()  # сам делает градиентный спуск (или другой метод)\n",
    "    print(f'iteration {i}, x1 {x1.item()}, x2 {x2.item()} loss {y.item()}, grad x1 {x1.grad.item()}, grad x2 {x2.grad.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответ: минимум достигается в точке $ (4, 2) $ и равен $ -32 $."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
